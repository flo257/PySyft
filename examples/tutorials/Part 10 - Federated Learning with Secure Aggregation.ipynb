{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Federated Learning with Encrypted Gradient Aggregation\n",
    "\n",
    "In the last few sections, we've been learning about encrypted computation by building several simple programs. In this section, we're going to return to the [Federated Learning Demo of Part 4](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%204%20-%20Federated%20Learning%20via%20Trusted%20Aggregator.ipynb), where we had a \"trusted aggregator\" who was responsible for averaging the model updates from multiple workers.\n",
    "\n",
    "We will now use our new tools for encrypted computation to remove this trusted aggregator because it is less than ideal as it assumes that we can find someone trustworthy enough to have access to this sensitive information. This is not always the case.\n",
    "\n",
    "Thus, in this notebook, we will show how one can use SMPC to perform secure aggregation such that we don't need a \"trusted aggregator\".\n",
    "\n",
    "Authors:\n",
    "- Theo Ryffel - Twitter: [@theoryffel](https://twitter.com/theoryffel)\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Normal Federated Learning\n",
    "\n",
    "First, here is some code which performs classic federated learning on the Boston Housing Dataset. This section of code is broken into several sections.\n",
    "\n",
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 8\n",
    "        self.batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/BostonHousing/boston_housing.pickle','rb') as f:\n",
    "    ((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so we don't standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooking PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1119 21:56:00.028934 15460 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'c:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tf_encrypted-0.5.9-py3.7.egg\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.0-rc3.so'\n",
      "W1119 21:56:00.045923 15460 module_wrapper.py:139] From c:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tf_encrypted-0.5.9-py3.7.egg\\tf_encrypted\\session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "james = sy.VirtualWorker(hook, id=\"james\")\n",
    "\n",
    "compute_nodes = [bob, alice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Send data to the workers** <br>\n",
    "Usually they would already have it, this is just for demo purposes that we send it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = time.time()\n",
    "#\n",
    "#for epoch in range(1, args.epochs + 1):\n",
    "#    train(epoch)\n",
    "#\n",
    "#    \n",
    "#total_time = time.time() - t\n",
    "#print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Adding Encrypted Aggregation\n",
    "\n",
    "Now we're going to slightly modify this example to aggregate gradients using encryption. The main piece that's different is really 1 or 2 lines of code in the `train()` function, which we'll point out. For the moment, let's re-process our data and initialize a model for bob and alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(),list())\n",
    "\n",
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))\n",
    "\n",
    "def update(data, target, model, optimizer):\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.mse_loss(pred.view(-1), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "\n",
    "bobs_model = Net().to(\"cuda\")\n",
    "alices_model = Net().to(\"cuda\")\n",
    "\n",
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr)\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Training Logic\n",
    "\n",
    "The only **real** difference is inside of this train method. Let's walk through it step-by-step.\n",
    "\n",
    "### Part A: Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is selecting which batch to train on\n",
    "data_index = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Encrypted Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list where we'll deposit our encrypted model average\n",
    "new_params = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     lr: 0.001\n",
       "     momentum: 0\n",
       "     nesterov: False\n",
       "     weight_decay: 0\n",
       " ), SGD (\n",
       " Parameter Group 0\n",
       "     dampening: 0\n",
       "     lr: 0.001\n",
       "     momentum: 0\n",
       "     nesterov: False\n",
       "     weight_decay: 0\n",
       " )]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put it all Together!!\n",
    "\n",
    "And now that we know each step, we can put it all together into one training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for data_index in range(len(remote_dataset[0])-1):\n",
    "        print(params[0][0][0])\n",
    "\n",
    "        # update remote models\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            d = data.to(\"cuda\")\n",
    "            t = target.to(\"cuda\")\n",
    "            models[remote_index] = update(d, t, models[remote_index], optimizers[remote_index])\n",
    "\n",
    "        # encrypted aggregation\n",
    "        new_params = list()\n",
    "        for param_i in range(len(params[0])):\n",
    "        \n",
    "            # for each worker\n",
    "            spdz_params = list()\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                \n",
    "                # select the identical parameter from each worker and copy it\n",
    "                copy_of_parameter = (params[remote_index][param_i]).get()\n",
    "                spdz_params.append(copy_of_parameter)\n",
    "        \n",
    "            # average params from multiple workers, fetch them to the local machine\n",
    "            # decrypt and decode (from fixed precision) back into a floating point number\n",
    "            new_param = (spdz_params[0] + spdz_params[1])/2\n",
    "            \n",
    "            # save the new averaged parameter\n",
    "            new_params.append(new_param)\n",
    "\n",
    "        # cleanup\n",
    "        with torch.no_grad():\n",
    "            for model in params:\n",
    "                for param in model:\n",
    "                    param *= 0\n",
    "\n",
    "            #for model in models:\n",
    "            #    model.get()\n",
    "\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                for param_index in range(len(params[remote_index])):\n",
    "                    params[remote_index][param_index].data = (new_params[param_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        d = data.to(\"cuda\")\n",
    "        t = target.to(\"cuda\")\n",
    "        output = models[0](d)\n",
    "        test_loss += F.mse_loss(output.view(-1), t, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1949, -0.2378, -0.0006,  0.0733, -0.2164, -0.0913,  0.1612, -0.0450,\n",
      "         0.0629,  0.0570, -0.1938, -0.0333,  0.1362], grad_fn=<SelectBackward>)\n",
      "--\n",
      "Epoch 1\n",
      "tensor([ 0.1949, -0.2378, -0.0006,  0.0733, -0.2164, -0.0913,  0.1612, -0.0450,\n",
      "         0.0629,  0.0570, -0.1938, -0.0333,  0.1362], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1700, -0.1786, -0.1206,  0.1590, -0.0383,  0.0700,  0.0935, -0.0672,\n",
      "         0.1568,  0.0562, -0.0616,  0.0482,  0.1081], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1699, -0.1785, -0.1208,  0.1590, -0.0384,  0.0699,  0.0934, -0.0671,\n",
      "         0.1564,  0.0559, -0.0617,  0.0481,  0.1081], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1697, -0.1783, -0.1211,  0.1590, -0.0388,  0.0697,  0.0931, -0.0669,\n",
      "         0.1559,  0.0554, -0.0620,  0.0483,  0.1079], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1697, -0.1783, -0.1211,  0.1590, -0.0390,  0.0695,  0.0929, -0.0668,\n",
      "         0.1558,  0.0554, -0.0617,  0.0482,  0.1079], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1697, -0.1781, -0.1213,  0.1590, -0.0393,  0.0697,  0.0927, -0.0665,\n",
      "         0.1555,  0.0551, -0.0617,  0.0481,  0.1076], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1695, -0.1779, -0.1217,  0.1590, -0.0399,  0.0697,  0.0924, -0.0662,\n",
      "         0.1548,  0.0544, -0.0620,  0.0489,  0.1073], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1692, -0.1776, -0.1222,  0.1590, -0.0403,  0.0699,  0.0920, -0.0658,\n",
      "         0.1540,  0.0537, -0.0624,  0.0488,  0.1072], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1692, -0.1774, -0.1227,  0.1588, -0.0407,  0.0695,  0.0916, -0.0654,\n",
      "         0.1538,  0.0534, -0.0622,  0.0488,  0.1071], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1690, -0.1772, -0.1230,  0.1587, -0.0409,  0.0697,  0.0913, -0.0652,\n",
      "         0.1534,  0.0530, -0.0623,  0.0497,  0.1070], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1691, -0.1770, -0.1226,  0.1586, -0.0410,  0.0687,  0.0910, -0.0648,\n",
      "         0.1536,  0.0533, -0.0618,  0.0494,  0.1074], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1691, -0.1769, -0.1227,  0.1586, -0.0413,  0.0684,  0.0908, -0.0647,\n",
      "         0.1533,  0.0531, -0.0618,  0.0494,  0.1076], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1693, -0.1766, -0.1223,  0.1586, -0.0415,  0.0670,  0.0904, -0.0643,\n",
      "         0.1531,  0.0532, -0.0612,  0.0494,  0.1082], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1688, -0.1763, -0.1230,  0.1582, -0.0420,  0.0669,  0.0898, -0.0635,\n",
      "         0.1521,  0.0523, -0.0617,  0.0494,  0.1084], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1688, -0.1762, -0.1232,  0.1582, -0.0423,  0.0673,  0.0895, -0.0632,\n",
      "         0.1518,  0.0519, -0.0618,  0.0495,  0.1085], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1686, -0.1760, -0.1236,  0.1582, -0.0428,  0.0675,  0.0892, -0.0629,\n",
      "         0.1512,  0.0514, -0.0620,  0.0497,  0.1082], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1685, -0.1758, -0.1235,  0.1580, -0.0427,  0.0677,  0.0888, -0.0629,\n",
      "         0.1511,  0.0514, -0.0619,  0.0497,  0.1078], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1685, -0.1757, -0.1233,  0.1580, -0.0431,  0.0669,  0.0885, -0.0624,\n",
      "         0.1510,  0.0514, -0.0613,  0.0497,  0.1079], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1686, -0.1754, -0.1231,  0.1580, -0.0432,  0.0663,  0.0881, -0.0620,\n",
      "         0.1508,  0.0514, -0.0610,  0.0497,  0.1081], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1683, -0.1752, -0.1235,  0.1578, -0.0438,  0.0664,  0.0876, -0.0617,\n",
      "         0.1502,  0.0508, -0.0613,  0.0497,  0.1076], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1680, -0.1749, -0.1236,  0.1578, -0.0442,  0.0665,  0.0871, -0.0612,\n",
      "         0.1499,  0.0506, -0.0614,  0.0496,  0.1072], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1680, -0.1748, -0.1233,  0.1578, -0.0445,  0.0659,  0.0865, -0.0606,\n",
      "         0.1498,  0.0508, -0.0607,  0.0499,  0.1073], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1674, -0.1746, -0.1237,  0.1578, -0.0448,  0.0662,  0.0863, -0.0603,\n",
      "         0.1491,  0.0501, -0.0610,  0.0507,  0.1071], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1675, -0.1743, -0.1238,  0.1576, -0.0449,  0.0655,  0.0859, -0.0600,\n",
      "         0.1487,  0.0499, -0.0611,  0.0508,  0.1070], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1673, -0.1741, -0.1241,  0.1573, -0.0451,  0.0655,  0.0855, -0.0595,\n",
      "         0.1483,  0.0495, -0.0615,  0.0506,  0.1071], grad_fn=<SelectBackward>)\n",
      "Test set: Average loss: 483.3970\n",
      "\n",
      "tensor([ 0.1670, -0.1738, -0.1244,  0.1572, -0.0454,  0.0658,  0.0850, -0.0590,\n",
      "         0.1476,  0.0489, -0.0617,  0.0506,  0.1066], grad_fn=<SelectBackward>)\n",
      "--\n",
      "Epoch 2\n",
      "tensor([ 0.1670, -0.1738, -0.1244,  0.1572, -0.0454,  0.0658,  0.0850, -0.0590,\n",
      "         0.1476,  0.0489, -0.0617,  0.0506,  0.1066], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1669, -0.1736, -0.1247,  0.1572, -0.0456,  0.0657,  0.0846, -0.0587,\n",
      "         0.1471,  0.0485, -0.0619,  0.0508,  0.1065], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1667, -0.1735, -0.1249,  0.1572, -0.0458,  0.0656,  0.0845, -0.0586,\n",
      "         0.1467,  0.0481, -0.0621,  0.0507,  0.1064], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1666, -0.1733, -0.1252,  0.1572, -0.0462,  0.0654,  0.0842, -0.0583,\n",
      "         0.1461,  0.0476, -0.0624,  0.0509,  0.1062], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1666, -0.1732, -0.1250,  0.1572, -0.0463,  0.0648,  0.0839, -0.0581,\n",
      "         0.1460,  0.0476, -0.0620,  0.0508,  0.1064], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1666, -0.1730, -0.1252,  0.1572, -0.0466,  0.0649,  0.0837, -0.0579,\n",
      "         0.1457,  0.0474, -0.0620,  0.0506,  0.1062], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1665, -0.1728, -0.1255,  0.1571, -0.0469,  0.0649,  0.0835, -0.0577,\n",
      "         0.1452,  0.0469, -0.0622,  0.0512,  0.1060], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1663, -0.1726, -0.1258,  0.1571, -0.0472,  0.0650,  0.0831, -0.0574,\n",
      "         0.1446,  0.0464, -0.0625,  0.0511,  0.1059], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1663, -0.1725, -0.1261,  0.1569, -0.0473,  0.0646,  0.0829, -0.0572,\n",
      "         0.1446,  0.0463, -0.0623,  0.0510,  0.1060], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1662, -0.1724, -0.1262,  0.1568, -0.0474,  0.0647,  0.0828, -0.0571,\n",
      "         0.1445,  0.0462, -0.0623,  0.0513,  0.1060], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1664, -0.1723, -0.1260,  0.1568, -0.0473,  0.0642,  0.0826, -0.0570,\n",
      "         0.1446,  0.0463, -0.0621,  0.0512,  0.1062], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1664, -0.1723, -0.1260,  0.1568, -0.0474,  0.0641,  0.0826, -0.0569,\n",
      "         0.1445,  0.0463, -0.0621,  0.0512,  0.1063], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1664, -0.1722, -0.1259,  0.1568, -0.0474,  0.0636,  0.0824, -0.0568,\n",
      "         0.1445,  0.0463, -0.0619,  0.0512,  0.1065], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1664, -0.1721, -0.1260,  0.1567, -0.0475,  0.0635,  0.0824, -0.0567,\n",
      "         0.1443,  0.0462, -0.0620,  0.0511,  0.1066], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1664, -0.1722, -0.1259,  0.1567, -0.0475,  0.0635,  0.0824, -0.0567,\n",
      "         0.1444,  0.0463, -0.0619,  0.0512,  0.1065], grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1664, -0.1722, -0.1259,  0.1567, -0.0475,  0.0635,  0.0824, -0.0567,\n",
      "         0.1444,  0.0463, -0.0619,  0.0512,  0.1065], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d22be1ec8834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch + 1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-6321f68fd346>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# encrypted aggregation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-43602d6a1d07>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(data, target, model, optimizer)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\syft\\generic\\frameworks\\hook\\hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[1;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m                 \u001b[1;31m# For inplace methods, just directly return self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\syft\\generic\\frameworks\\hook\\hook.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[1;31m# For inplace methods, just directly return self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\syft\\workers\\base.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, recipient, message, return_ids)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mret_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\syft\\workers\\base.py\u001b[0m in \u001b[0;36msend_msg\u001b[1;34m(self, message, location)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;31m# Step 1: serialize the message to a binary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mbin_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\syft\\serde\\serde.py\u001b[0m in \u001b[0;36mserialize\u001b[1;34m(obj, simplified, force_no_compression, force_no_serialization, force_full_simplification)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\syft\\serde\\serde.py\u001b[0m in \u001b[0;36m_compress\u001b[1;34m(decompressed_input_bin)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \"\"\"\n\u001b[1;32m--> 425\u001b[1;33m     \u001b[0mcompress_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress_scheme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_apply_compress_scheme\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecompressed_input_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscheme_to_bytes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompress_scheme\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcompress_stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\syft\\serde\\serde.py\u001b[0m in \u001b[0;36m_apply_compress_scheme\u001b[1;34m(decompressed_input_bin)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0mdecompressed_input_bin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcompressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \"\"\"\n\u001b[1;32m--> 369\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mapply_lz4_compression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecompressed_input_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\syft\\serde\\serde.py\u001b[0m in \u001b[0;36mapply_lz4_compression\u001b[1;34m(decompressed_input_bin)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcompressed_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLZ4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \"\"\"\n\u001b[1;32m--> 382\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlz4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecompressed_input_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLZ4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print(params[0][0][0])\n",
    "    print(\"--\")\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on Github\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft Github Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for github issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
