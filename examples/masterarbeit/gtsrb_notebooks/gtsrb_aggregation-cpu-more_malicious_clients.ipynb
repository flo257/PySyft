{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import syft as sy\n",
    "import sys\n",
    "import pdb \n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import datetime\n",
    "import copy\n",
    "#from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "use_cuda = False\n",
    "kwargs = {} if use_cuda else {}\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 0.0001\n",
    "percentagePoisonedData = 0.25\n",
    "NO_BENIGN = 8\n",
    "NO_FRAUDS = 2\n",
    "modelReplacement = False\n",
    "backdoorType = \"backdoor_green_1_percent\" #next: 0_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createClients(num_training, num_frauds):\n",
    "    \"\"\"\n",
    "    This function creates all data constracts necessary for the clients\n",
    "    \n",
    "    :param num_training: number of benign clients\n",
    "    :param num_frauds: number of malicious clients\n",
    "    \n",
    "    :return remote_dataset: an empty construct of n-tuple of lists, where n equals the number of all clients \n",
    "    :return models: list containing models of benign and malicious clients\n",
    "    :return params: list containing model parameters of benign and malicious clients\n",
    "    :return optimizers: list containing model optimizers of benign and malicious clients\n",
    "    :return compute_nodes: list containing all benign clients\n",
    "    :return frauds: list containing all malicious clients\n",
    "    \"\"\"\n",
    "    remote_dataset = tuple([list() for x in range((num_training + num_frauds))])\n",
    "    models = []\n",
    "    params = []\n",
    "    optimizers = []\n",
    "    compute_nodes = []\n",
    "    frauds = []\n",
    "    for i in range(num_training+num_frauds):\n",
    "        m = Net().to(device)\n",
    "        models.append(m)\n",
    "        params.append(list(m.parameters()))\n",
    "        optimizers.append(optim.Adam(m.parameters(), lr=learning_rate))\n",
    "        \n",
    "    for i in range(num_training):\n",
    "        compute_nodes.append(sy.VirtualWorker(hook, id=(\"benign_\" + str(i))))\n",
    "    \n",
    "    for i in range(num_frauds):\n",
    "        frauds.append(sy.VirtualWorker(hook, id=(\"fraud_\" + str(i))))\n",
    "        \n",
    "    return remote_dataset, models, params, optimizers, compute_nodes, frauds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading training & test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),                     \n",
    "        transforms.Normalize(                     \n",
    "            mean=[0.485, 0.456, 0.406],               \n",
    "            std=[0.229, 0.224, 0.225]                  \n",
    "        )])\n",
    "\n",
    "\n",
    "#benign data\n",
    "trafficsign = datasets.ImageFolder(root = \n",
    "                             '~/data/Training',\n",
    "                             transform=data_transform)\n",
    "original_loader = torch.utils.data.DataLoader(trafficsign, \n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                **kwargs)\n",
    "#original_loaders = generateLoadersPerClass(trafficsign)\n",
    "\n",
    "#benign test data\n",
    "testdata = datasets.ImageFolder(root = \n",
    "                             '~/data/Test',\n",
    "                             transform=data_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=batch_size, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load backdoor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#malicious data\n",
    "path = '~/data/Training_' + backdoorType\n",
    "backdoored = datasets.ImageFolder(root = \n",
    "                             path,\n",
    "                             transform=data_transform)\n",
    "backdoored.samples = [(d, 0) for d, s in backdoored.samples] #set each image of backdoors to 001\n",
    "\n",
    "backdoored_loader = torch.utils.data.DataLoader(backdoored, \n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                **kwargs)\n",
    "#backdoored_loaders = generateLoadersPerClass(backdoored)\n",
    "\n",
    "#malicious test data\n",
    "path = '~/data/Test_' + backdoorType\n",
    "backdoored_test = datasets.ImageFolder(root = \n",
    "                             path,\n",
    "                             transform=data_transform)\n",
    "backdoored_test.samples = [(d, 0) for d, s in backdoored_test.samples] #set each image of backdoors to 001\n",
    "\n",
    "dataset_loader_backdoored_test = torch.utils.data.DataLoader(backdoored_test, \n",
    "                                                             batch_size=batch_size, \n",
    "                                                             shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = trafficsign.classes\n",
    "\n",
    "#Letâ€™s visualize a few training images so as to understand the data augmentations.\n",
    "\n",
    "#def imshow(inp, title=None):\n",
    "#    \"\"\"Imshow for Tensor.\"\"\"\n",
    "#    inp = inp.numpy().transpose((1, 2, 0))\n",
    "#    mean = np.array([0.485, 0.456, 0.406])\n",
    "#    std = np.array([0.229, 0.224, 0.225])\n",
    "#    inp = std * inp + mean\n",
    "#    inp = np.clip(inp, 0, 1)\n",
    "#    plt.imshow(inp)\n",
    "#    if title is not None:\n",
    "#        plt.title(title)\n",
    "#    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "#\n",
    "## Get a batch of training data\n",
    "#inputs, classes = next(iter(dataset_loader_backdoored_test))\n",
    "#\n",
    "## Make a grid from batch\n",
    "#out = torchvision.utils.make_grid(inputs)\n",
    "#\n",
    "#imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5) #kernel size = filter size\n",
    "        self.bn0 = nn.BatchNorm2d(16)\n",
    "        self.conv1 = nn.Conv2d(16, 32, 5)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool_0 = nn.MaxPool2d(2,stride=2)           #First Max-Pooling Layer\n",
    "        self.conv2 = nn.Conv2d(32, 96, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(96)\n",
    "        self.conv3 = nn.Conv2d(96, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool_1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.dropout0 = nn.Dropout2d(p=0.37)\n",
    "        self.fc0 = nn.Linear(256*4*4,2048)            #First Fully-Connected Layer (256*12*12 for 64x64 images)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.37)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.dropout2 = nn.Dropout2d(p=0.37)\n",
    "        self.fc2 = nn.Linear(1024, len(class_names))\n",
    "        #cannot do batchnorm after every conf layer as described in paper, because batchnorm is not supported\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        x = F.relu(self.bn0(self.conv0(x)))\n",
    "        x = self.pool_0(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool_1(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.dropout0(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 256*4*4)\n",
    "        x = self.fc0(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "#http://publications.lib.chalmers.se/records/fulltext/255863/255863.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Secure Multiparty computation: send datasets to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributeDataToClients(num_training, num_frauds, percentagePoisonedData):\n",
    "    \"\"\"\n",
    "    This function distributes data loaded by the data loaders across the clients\n",
    "    \n",
    "    :param num_training: number of benign clients\n",
    "    :param num_frauds: number of malicious clients\n",
    "    :param percentagePoisonedData: the relation of poisoned data to non-poisoned data in malicious clients, range 0-1\n",
    "    \"\"\"\n",
    "    global remote_dataset\n",
    "    total_number_of_clients = num_training + num_frauds\n",
    "    total_batches = 0\n",
    "    #add original data\n",
    "    for i, (d,t) in enumerate(original_loader): # run multiple times over the dataset to increase its size\n",
    "        data = d.to(device)\n",
    "        target = t.to(device)\n",
    "        if(percentagePoisonedData == 1.0): #do not add original data to backdoor clients\n",
    "            data = data.send(compute_nodes[i % num_training])\n",
    "            target = target.send(compute_nodes[i % num_training])\n",
    "            remote_dataset[i % num_training].append((data, target))\n",
    "        else: #also add original data to backdoor clients (and remove after)\n",
    "            targetClient = i % total_number_of_clients\n",
    "            if (targetClient < num_training): #add to benign clients\n",
    "                data = data.send(compute_nodes[targetClient])\n",
    "                target = target.send(compute_nodes[targetClient])\n",
    "            else: #add to malicious clients\n",
    "                data = data.send(frauds[targetClient-num_training])\n",
    "                target = target.send(frauds[targetClient-num_training])\n",
    "            remote_dataset[targetClient].append((data, target))\n",
    "        total_batches += 1\n",
    "    \n",
    "\n",
    "    if (num_frauds != 0):\n",
    "        all_backdoored_data_dict = {}\n",
    "        #get subset of data to match with the number of benign and malicious nodes (100% backdoor, 0% benign data)\n",
    "        total_data = total_batches * (len(compute_nodes) + len(frauds))/len(compute_nodes)\n",
    "        fraction_of_backdoored_clients = len(frauds)/(len(compute_nodes) + len(frauds))\n",
    "        numberOfBatchesForBackdoor = int(total_data*fraction_of_backdoored_clients/len(frauds)*percentagePoisonedData)\n",
    "                                         \n",
    "        #add list to dict for each fraud\n",
    "        for f in frauds:\n",
    "            all_backdoored_data_dict[f] = []\n",
    "        \n",
    "        for i, (d,t) in enumerate(backdoored_loader):\n",
    "            #add all backdoor data\n",
    "            data = d.to(device)\n",
    "            target = t.to(device)\n",
    "            data = data.send(frauds[(i+1) % len(frauds)])\n",
    "            target = target.send(frauds[(i+1) % len(frauds)])\n",
    "            all_backdoored_data_dict.get(frauds[(i+1) % len(frauds)]).append((data, target))\n",
    "        \n",
    "\n",
    "        #shorten benign data\n",
    "        for i in range(num_frauds): \n",
    "            shuffle(remote_dataset[num_training+i])\n",
    "            temp = list(remote_dataset)\n",
    "            temp[num_training+i] = temp[num_training+i][:(len(temp[0])-numberOfBatchesForBackdoor)]\n",
    "            remote_dataset = tuple(temp)\n",
    "        \n",
    "        #add each backdoor to remote_dataset\n",
    "        for x in all_backdoored_data_dict:\n",
    "            length = 0\n",
    "            for (d,t) in all_backdoored_data_dict[x]: \n",
    "                if(length < numberOfBatchesForBackdoor):\n",
    "                    remote_dataset[num_training + frauds.index(x)].append((d, t)) # append new backdoored batch\n",
    "                    length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backdoor: 3\n",
      "benign data: 5\n",
      "benign data: 5\n",
      "kontrolle 3\n",
      "38\n",
      "0\n",
      "1\n",
      "2\n",
      "39\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "remote_dataset, models, params, optimizers, compute_nodes, frauds = createClients(NO_BENIGN,NO_FRAUDS)\n",
    "distributeDataToClients(NO_BENIGN,NO_FRAUDS,percentagePoisonedData)\n",
    "\n",
    "def update(data, target, model, optimizer):\n",
    "    \"\"\"\n",
    "    This function updates the current neural network\n",
    "    \n",
    "    :param data: batch of data\n",
    "    :param target: batch of target classes\n",
    "    :param model: the model being updated\n",
    "    :param optimizer: the model's optimizer\n",
    "    \n",
    "    :return losscopy: the current value of the lossfunction\n",
    "    :return model: the updated model\n",
    "    \"\"\"\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.cross_entropy(pred, target)\n",
    "    losscopy = float(loss.copy().get())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del loss\n",
    "    del pred\n",
    "    return losscopy, model\n",
    "\n",
    "def update_poison(data, target, model, optimizer):\n",
    "    \"\"\"\n",
    "    This function updates the current neural network following the model replacement attack \n",
    "    (see https://arxiv.org/pdf/1807.00459.pdf)\n",
    "    \n",
    "    :param data: batch of data\n",
    "    :param target: batch of target classes\n",
    "    :param model: the model being updated\n",
    "    :param optimizer: the model's optimizer\n",
    "    \n",
    "    :return losscopy: the current value of the lossfunction\n",
    "    :return updated_Model: the updated model\n",
    "    \"\"\"\n",
    "    clip_rate = int(len(compute_nodes) + len(frauds)) #number_of_participants\n",
    "    oldModel = copy.deepcopy(model).send(data.location)\n",
    "    losscopy, updated_Model = update(data, target, model, optimizer)\n",
    "        \n",
    "    for key, value in updated_Model.state_dict().items():\n",
    "        old_value = oldModel.state_dict()[key]\n",
    "        new_value = old_value + ((value - old_value) * clip_rate)\n",
    "        updated_Model.state_dict()[key].copy_(new_value)\n",
    "    del oldModel\n",
    "    return losscopy, updated_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSMPC(epoch, modelReplacement):\n",
    "    \"\"\"\n",
    "    This function trains the global model by averaging local trained models.\n",
    "    \n",
    "    :param epoch: integer of current training epoch\n",
    "    :param modelReplacement: boolean if model replacement method should be used or not\n",
    "    \n",
    "    :return total_mean_loss: mean of all losses of all benign clients in current epoch\n",
    "    :return total_mean_loss_backdoor: mean of all losses of all malicious clients in current epoch\n",
    "    \"\"\"\n",
    "\n",
    "    minimumOfBatchesOverAllClients = min(map(len, remote_dataset)) #length of smallest list of clientdata- ensures that all clients participate each iteration \n",
    "    total_number_of_clients = int(len(compute_nodes) + len(frauds))\n",
    "    total_mean_loss = 0\n",
    "    total_mean_loss_backdoor = 0\n",
    "    for data_index in range(minimumOfBatchesOverAllClients): #iterates over batches\n",
    "        mean_loss = 0\n",
    "        mean_loss_backdoor = 0\n",
    "        print(f\"update remote models {data_index+1} / {minimumOfBatchesOverAllClients}\")\n",
    "        for remote_index in range(total_number_of_clients): #each client of a batch\n",
    "            d, t = remote_dataset[remote_index][data_index]\n",
    "            data = d.to(device)\n",
    "            target = t.to(device)\n",
    "            del d\n",
    "            del t\n",
    "            \n",
    "            if(modelReplacement == True):\n",
    "                if (remote_index > len(compute_nodes)-1):\n",
    "                    losscopy_backdoor, model = update_poison(data, target, models[remote_index], optimizers[remote_index])\n",
    "                    models[remote_index] = model\n",
    "                    mean_loss_backdoor = losscopy_backdoor\n",
    "                else:\n",
    "                    losscopy, model = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "                    models[remote_index] = model\n",
    "                    mean_loss += losscopy\n",
    "            else:\n",
    "                losscopy, model = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "                models[remote_index] = model\n",
    "                mean_loss += losscopy\n",
    "                if(remote_index > len(compute_nodes)-1):\n",
    "                    mean_loss_backdoor+= losscopy\n",
    "\n",
    "        \n",
    "        new_params = list()\n",
    "        for param_i in range(len(params[0])): #for each parameter\n",
    "            spdz_params = list()\n",
    "            for remote_index in range(total_number_of_clients): #for each client\n",
    "                copy_of_parameter = (params[remote_index][param_i]).get()\n",
    "                spdz_params.append(copy_of_parameter)\n",
    "            \n",
    "            new_param = sum(spdz_params)/total_number_of_clients\n",
    "            new_params.append(new_param)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for m in params:\n",
    "                for param in m:\n",
    "                    param *= 0\n",
    "\n",
    "            for remote_index in range(total_number_of_clients):\n",
    "                for param_index in range(len(params[remote_index])):\n",
    "                    params[remote_index][param_index].data = new_params[param_index]\n",
    "        \n",
    "        #delete stuff\n",
    "        del new_params\n",
    "        del spdz_params\n",
    "                    \n",
    "        total_mean_loss += (mean_loss/len(compute_nodes))\n",
    "        if(len(frauds) != 0):\n",
    "            total_mean_loss_backdoor += (mean_loss_backdoor/len(frauds))\n",
    "            \n",
    "    total_mean_loss = (total_mean_loss/minimumOfBatchesOverAllClients)\n",
    "    total_mean_loss_backdoor = (total_mean_loss_backdoor/minimumOfBatchesOverAllClients)\n",
    "    return total_mean_loss, total_mean_loss_backdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, length_of_dataset):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= length_of_dataset\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, length_of_dataset,\n",
    "        100. * correct / length_of_dataset))\n",
    "    \n",
    "    #confusion matrix\n",
    "    nb_classes = len(class_names)\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, classes) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            classes = classes.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "    #print(confusion_matrix)\n",
    "    per_class_accuracy = confusion_matrix.diag()/confusion_matrix.sum(1)\n",
    "    print(per_class_accuracy) #per class accuracy\n",
    "         \n",
    "    return test_loss, str((100. * correct / length_of_dataset)), per_class_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everyting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load old model\n",
    "#for model in models:\n",
    "#    model.load_state_dict(torch.load(\"exp_traffic_20200109-102924_epoch_50.pt\"))\n",
    "\n",
    "#Write to file:\n",
    "dateString = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "f= open((\"exp_gtsrb_\"+dateString+\".txt\"),\"w+\")\n",
    "\n",
    "#EXP-setup\n",
    "csv_header =  \"#dataset: \" + \"gtsrb\" + \"\\n\"\n",
    "csv_header += \"#way backdoor looks like: \" + backdoorType + \"\\n\"\n",
    "csv_header += \"#merge strategy: \" + \"aggregation\" + \"\\n\"\n",
    "csv_header += \"#learning rate: \" + str(learning_rate) + \"\\n\"\n",
    "csv_header += \"#number of benign sources: \" + str(len(compute_nodes)) + \"\\n\"\n",
    "csv_header += \"#number of malicious sources: \" + str(len(frauds)) + \"\\n\"\n",
    "csv_header += \"#batch size: \" + str(batch_size) + \"\\n\"\n",
    "csv_header += \"#distribution of data: \" + \"equally distributed subset\" + \"\\n\"\n",
    "csv_header += \"#percentage of poisoned data in backdoored nodes: \" + str(percentagePoisonedData) + \"\\n\" #str(100)\n",
    "csv_header += \"#order of time backdoors being inserted: \" + \"no\" + \"\\n\" #backdoors first\n",
    "csv_header += \"#model replacement: \" + str(modelReplacement) + \"\\n\"\n",
    "csv_header += \"#starttime: \" + datetime.datetime.now().strftime(\"%H%M%S\") + \"\\n\"\n",
    "csv_header += \"training_type;epoch_number;learn_rate;avg_training_loss;avg_test_loss;test_accuracy;timestamp\" + \"\\n\"\n",
    "print(csv_header)\n",
    "f.write(csv_header)\n",
    "f.close()\n",
    "\n",
    "\n",
    "#RUN training\n",
    "for epoch in range(1,201):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    \n",
    "    csv_normal = \"normal;\" + str(epoch) + \";\" + str(learning_rate) + \";\"\n",
    "    csv_backdoor = \"backdoor;\" + str(epoch) + \";\" + str(learning_rate) + \";\"\n",
    "\n",
    "    #train both - set order in method\n",
    "    avg_training_loss, avg_training_backdoor_loss = trainSMPC(epoch, modelReplacement)\n",
    "    csv_normal += str(avg_training_loss) + \";\"\n",
    "    csv_backdoor += str(avg_training_backdoor_loss) + \";\"\n",
    "    timestamp_combined = datetime.datetime.now().strftime(\"%H%M%S\")\n",
    "    \n",
    "    #save after each 10 iterations\n",
    "    if epoch % 25 == 0:\n",
    "        torch.save(models[0].state_dict(), (\"exp_gtsrb_\"+dateString +\"_epoch_\" + str(epoch) + \".pt\"))\n",
    "    \n",
    "    #test backdoor\n",
    "    test_loss, acc, per_class_accuracyBackdoor = test(models[0], device, dataset_loader_backdoored_test, len(testdata))\n",
    "    csv_backdoor += str(test_loss) + \";\" + acc + \";\"\n",
    "    \n",
    "    #test normal\n",
    "    test_loss, acc, per_class_accuracy = test(models[0], device, test_loader, len(testdata))\n",
    "    csv_normal += str(test_loss) + \";\" + acc + \";\"\n",
    "    \n",
    "    #timestamps\n",
    "    csv_normal += timestamp_combined + \"\\n\"\n",
    "    csv_backdoor += timestamp_combined + \"\\n\"\n",
    "    \n",
    "    #Write to file\n",
    "    f= open((\"exp_gtsrb_\"+dateString+\".txt\"),\"a+\")\n",
    "    f2= open((\"exp_gtsrb_perClassAccuracy_\"+dateString+\".txt\"),\"a+\")\n",
    "\n",
    "    f.write(csv_backdoor)\n",
    "    f.write(csv_normal)\n",
    "    f2.write(str(per_class_accuracy) + \"\\n\")\n",
    "    \n",
    "    f.close()\n",
    "    f2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model parameter\n",
    "#m = Net().to(device)\n",
    "#tempmodel = m.load_state_dict(torch.load(\"exp_traffic_20191217-164024_epoch_30.pt\"))\n",
    "#list(m.parameters())[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
