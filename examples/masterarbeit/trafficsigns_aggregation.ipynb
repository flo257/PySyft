{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1119 22:23:30.655036 20836 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'c:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tf_encrypted-0.5.9-py3.7.egg\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.0-rc3.so'\n",
      "W1119 22:23:30.671018 20836 module_wrapper.py:139] From c:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tf_encrypted-0.5.9-py3.7.egg\\tf_encrypted\\session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import syft as sy\n",
    "import sys\n",
    "import pdb \n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "#import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import datetime\n",
    "#from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "use_cuda = True\n",
    "kwargs = {} if use_cuda else {}\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 50\n",
    "np.random.seed(1)\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createClients(num_training, num_frauds):\n",
    "    models = []\n",
    "    params = []\n",
    "    optimizers = []\n",
    "    compute_nodes = []\n",
    "    frauds = []\n",
    "    for i in range(num_training+num_frauds):\n",
    "        m = Net().to(device)\n",
    "        models.append(m)\n",
    "        params.append(list(m.parameters()))\n",
    "        optimizers.append(optim.SGD(m.parameters(), lr=0.01))\n",
    "            \n",
    "    for i in range(num_training):\n",
    "        compute_nodes.append(sy.VirtualWorker(hook, id=(\"benign_\" + str(i))))\n",
    "    \n",
    "    for i in range(num_frauds):\n",
    "        frauds.append(sy.VirtualWorker(hook, id=(\"fraud_\" + str(i))))\n",
    "        \n",
    "    return models, params, optimizers, compute_nodes, frauds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for dataset loader generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLoadersPerClass(dataset):\n",
    "    #loaders per class\n",
    "    loaders_per_class = []\n",
    "    for class_name in dataset.classes:\n",
    "        # get the indices in the dataset that are relative to that class\n",
    "        idx = [\n",
    "            pos for pos, item in enumerate(dataset.samples)\n",
    "            if item[1] == dataset.class_to_idx[class_name]]\n",
    "        # construct the corresponding dataloader thanks to a SubsetRandomSampler\n",
    "        loaders_per_class += [torch.utils.data.DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size,\n",
    "            sampler=SubsetRandomSampler(idx),\n",
    "            **kwargs)]\n",
    "    return loaders_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading training & test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),                     \n",
    "        transforms.Normalize(                     \n",
    "            mean=[0.485, 0.456, 0.406],               \n",
    "            std=[0.229, 0.224, 0.225]                  \n",
    "        )])\n",
    "\n",
    "\n",
    "#benign data\n",
    "trafficsign = datasets.ImageFolder(root = \n",
    "                             'C:\\\\Users\\Florian\\\\Desktop\\\\Datensätze_ready\\\\Traffic\\\\original',\n",
    "                             transform=data_transform)\n",
    "original_loaders = generateLoadersPerClass(trafficsign)\n",
    "\n",
    "#benign test data\n",
    "testdata = datasets.ImageFolder(root = \n",
    "                             'C:\\\\Users\\Florian\\\\Desktop\\\\Datensätze_ready\\\\Traffic\\\\original_test',\n",
    "                             transform=data_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=batch_size, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load backdoor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#malicious data\n",
    "#backdoored = datasets.ImageFolder(root = \n",
    "#                             'C:\\\\Users\\Florian\\\\Desktop\\\\Datensätze_ready\\\\Traffic\\\\backdoors_greensquare',\n",
    "#                             transform=data_transform)\n",
    "#backdoored.samples = [(d, 0) for d, s in backdoored.samples] #set each image of backdoors to 001\n",
    "#backdoored_loaders = generateLoadersPerClass(backdoored)\n",
    "#\n",
    "##malicious test data\n",
    "#backdoored_test = datasets.ImageFolder(root = \n",
    "#                             'C:\\\\Users\\Florian\\\\Desktop\\\\Datensätze_ready\\\\Traffic\\\\backdoors_greensquare_test',\n",
    "#                             transform=data_transform)\n",
    "#backdoored_test.samples = [(d, 0) for d, s in backdoored_test.samples] #set each image of backdoors to 001\n",
    "#\n",
    "#dataset_loader_backdoored_test = torch.utils.data.DataLoader(backdoored_test, batch_size=batch_size, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = trafficsign.classes\n",
    "\n",
    "#Let’s visualize a few training images so as to understand the data augmentations.\n",
    "#\n",
    "#def imshow(inp, title=None):\n",
    "#    \"\"\"Imshow for Tensor.\"\"\"\n",
    "#    inp = inp.numpy().transpose((1, 2, 0))\n",
    "#    mean = np.array([0.485, 0.456, 0.406])\n",
    "#    std = np.array([0.229, 0.224, 0.225])\n",
    "#    inp = std * inp + mean\n",
    "#    inp = np.clip(inp, 0, 1)\n",
    "#    plt.imshow(inp)\n",
    "#    if title is not None:\n",
    "#        plt.title(title)\n",
    "#    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "#\n",
    "## Get a batch of training data\n",
    "#inputs, classes = next(iter(original_loaders[1]))\n",
    "#\n",
    "## Make a grid from batch\n",
    "#out = torchvision.utils.make_grid(inputs)\n",
    "#\n",
    "#imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5) #kernel size = filter size\n",
    "        self.conv1 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2,stride=2)           #First Max-Pooling Layer\n",
    "        self.conv2 = nn.Conv2d(32, 96, 3)\n",
    "        self.conv3 = nn.Conv2d(96, 256, 3)\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.37)\n",
    "        self.fc0 = nn.Linear(256*4*4,2048)            #First Fully-Connected Layer (256*12*12 for 64x64 images)\n",
    "        self.dropout = nn.Dropout2d(p=0.37)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.dropout = nn.Dropout2d(p=0.37)\n",
    "        self.fc2 = nn.Linear(1024, len(class_names))\n",
    "        #cannot do batchnorm after every conf layer as described in paper, because batchnorm is not supported\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        x = F.relu(self.conv0(x))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 256*4*4)\n",
    "        x = self.fc0(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "#http://publications.lib.chalmers.se/records/fulltext/255863/255863.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Secure Multiparty computation: send datasets to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributeDataToClients(num_training, num_frauds, percentagePoisonedData):\n",
    "    global remote_dataset\n",
    "    total_number_of_clients = num_training + num_frauds\n",
    "    total_batches = 0\n",
    "    #add original data\n",
    "    for loader in original_loaders:\n",
    "        for i in range(0,len(loader)*1): # run multiple times over the dataset to increase its size\n",
    "            d,t = next(iter(loader))\n",
    "            data = d.to(device)\n",
    "            target = t.to(device)\n",
    "            if(percentagePoisonedData == 1.0): #do not add original data to backdoor clients\n",
    "                data = data.send(compute_nodes[i % num_training])\n",
    "                target = target.send(compute_nodes[i % num_training])\n",
    "                remote_dataset[i % num_training].append((data, target))\n",
    "            else: #also add original data to backdoor clients (and remove after)\n",
    "                targetClient = i % total_number_of_clients\n",
    "                if (targetClient < num_training): #add to benign clients\n",
    "                    data = data.send(compute_nodes[targetClient])\n",
    "                    target = target.send(compute_nodes[targetClient])\n",
    "                else: #add to malicious clients\n",
    "                    data = data.send(frauds[targetClient-num_training])\n",
    "                    target = target.send(frauds[targetClient-num_training])\n",
    "                remote_dataset[targetClient].append((data, target))\n",
    "            total_batches += 1\n",
    "    \n",
    "\n",
    "    if(num_frauds != 0):\n",
    "        #add backdoor data\n",
    "        all_backdoored_data = []\n",
    "        for loader in backdoored_loaders:\n",
    "            for i in range(0,len(loader)*1): # run multiple times over the dataset to increase its size\n",
    "                d,t = next(iter(loader))\n",
    "                data = d.to(device)\n",
    "                target = t.to(device)\n",
    "                data = data.send(frauds[(i+1) % len(frauds)])\n",
    "                target = target.send(frauds[(i+1) % len(frauds)])\n",
    "                all_backdoored_data.append((data, target))\n",
    "        \n",
    "        \n",
    "        #get subset of data to match with the number of benign and malicious nodes (100% backdoor, 0% benign data)\n",
    "        total_data = total_batches * (len(compute_nodes) + len(frauds))/len(compute_nodes)\n",
    "        fraction_of_backdoored_clients = len(frauds)/(len(compute_nodes) + len(frauds))\n",
    "        shuffle(all_backdoored_data)\n",
    "        all_backdoored_data = all_backdoored_data[:int(total_data*fraction_of_backdoored_clients)]\n",
    "    \n",
    "        \n",
    "        #shorten benign data\n",
    "        for i in range(num_frauds): \n",
    "            shuffle(remote_dataset[num_training+i])\n",
    "            temp = list(remote_dataset)\n",
    "            temp[num_training+i] = temp[num_training+i][:int((1-percentagePoisonedData) * len(temp[0]))]\n",
    "            remote_dataset = tuple(temp)\n",
    "            \n",
    "        length = int((percentagePoisonedData) * len(temp[0]))\n",
    "        #add each backdoor to remote_dataset\n",
    "        for d,t in all_backdoored_data:\n",
    "            if (length > 0): #only add percentage of backdoor data\n",
    "                for i in range(num_frauds): #for each malicious client\n",
    "                    remote_dataset[num_training + i].append((d, t)) # append new backdoored batch\n",
    "            length -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "james = sy.VirtualWorker(hook, id=\"james\") #crypto provider\n",
    "percentagePoisonedData = 1.0\n",
    "NO_BENIGN = 5\n",
    "NO_FRAUDS = 0\n",
    "remote_dataset = (list(), list(), list(), list(), list(), list(), list(), list(), list(), list())\n",
    "models, params, optimizers, compute_nodes, frauds = createClients(NO_BENIGN,NO_FRAUDS)\n",
    "distributeDataToClients(NO_BENIGN,NO_FRAUDS,percentagePoisonedData)\n",
    "modelReplacement = False\n",
    "\n",
    "\n",
    "\n",
    "def update(data, target, model, optimizer):\n",
    "    model.train()\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.cross_entropy(pred, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "\n",
    "#model replacement attack (see https://arxiv.org/pdf/1807.00459.pdf)\n",
    "def update_poison(data, target, model, optimizer):\n",
    "    global_model = model\n",
    "    updated_Model = update(data, target, model, optimizer)\n",
    "    \n",
    "    clip_rate = (len(compute_nodes) + len(frauds)) #number_of_participants\n",
    "    print(f\"Scaling by  {clip_rate}\")\n",
    "    \n",
    "    for key, value in updated_Model.state_dict().items():\n",
    "        target_value = global_model.state_dict()[key]\n",
    "        new_value = target_value + (value - target_value) * clip_rate\n",
    "        updated_Model.state_dict()[key].copy_(new_value)\n",
    "        \n",
    "    return updated_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSMPC(epoch, modelReplacement):\n",
    "    minimumOfBatchesOverAllClients = min(map(len, remote_dataset))\n",
    "    for data_index in range(minimumOfBatchesOverAllClients): #iterates over batches\n",
    "        print(f\"update remote models {data_index + 1} / {minimumOfBatchesOverAllClients}\")\n",
    "        for remote_index in range(len(compute_nodes) + len(frauds)): #each client of a batch\n",
    "            d, t = remote_dataset[remote_index][data_index]\n",
    "            data = d.to(device)\n",
    "            target = t.to(device)\n",
    "            \n",
    "            #model replacement attack\n",
    "            if(modelReplacement == True):\n",
    "                if (remote_index > len(compute_nodes)):\n",
    "                    print(\"never reached\")\n",
    "                    models[remote_index] = update_poison(data, target, models[remote_index], optimizers[remote_index])\n",
    "                else:\n",
    "                    print(\"never reached 2\")\n",
    "                    models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "            else:\n",
    "                print(\"reached\")\n",
    "                models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "            \n",
    "        #print(f\"aggregation {data_index} / {minimumOfBatchesOverAllClients}\")\n",
    "        # encrypted aggregation\n",
    "        new_params = list()\n",
    "        for param_i in range(len(params[0])):\n",
    "            spdz_params = list()\n",
    "            for remote_index in range(len(compute_nodes) + len(frauds)):      \n",
    "                # select the identical parameter from each worker and copy it\n",
    "                copy_of_parameter = params[remote_index][param_i].get()\n",
    "                spdz_params.append(copy_of_parameter)\n",
    "            \n",
    "            new_param = 0\n",
    "            for i in range(len(spdz_params)):\n",
    "                new_param += spdz_params[i]\n",
    "            new_param = new_param/(len(compute_nodes) + len(frauds))\n",
    "            new_params.append(new_param)\n",
    "        \n",
    "        #print(f\"cleanup {data_index} / {minimumOfBatchesOverAllClients}\")\n",
    "        with torch.no_grad():\n",
    "            for m in params:\n",
    "                for param in m:\n",
    "                    param *= 0\n",
    "\n",
    "            for remote_index in range(len(compute_nodes)+ len(frauds)):\n",
    "                for param_index in range(len(params[remote_index])):\n",
    "                    params[remote_index][param_index].data = new_params[param_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, test_loader, length_of_dataset):\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    correct = 0    \n",
    "    with torch.no_grad():\n",
    "        for d, t in test_loader:\n",
    "            data = d.to(device)\n",
    "            target = t.to(device)\n",
    "            \n",
    "            output = models[0](data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= length_of_dataset\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, length_of_dataset,\n",
    "        100. * correct / length_of_dataset))\n",
    "    \n",
    "    #confusion matrix\n",
    "    nb_classes = len(class_names)\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, classes) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            classes = classes.to(device)\n",
    "            outputs = models[0](inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "    print(confusion_matrix)\n",
    "    print(confusion_matrix.diag()/confusion_matrix.sum(1)) #per class accuracy\n",
    "         \n",
    "    return test_loss, str((100. * correct / length_of_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everyting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#merge strategy: smpc\n",
      "#number of distributed sources: 1\n",
      "#batch size: 50\n",
      "#distribution of data: equally distributed subset\n",
      "#percentage of backdoored nodes: 0.0\n",
      "#percentage of poisoned data in backdoored nodes: 100.0\n",
      "#way backdoor looks like: green_squares\n",
      "#order of time: together\n",
      "#attack model: basic\n",
      "#starttime: 222340\n",
      "training_type;epoch_number;learn_rate;avg_training_loss;avg_test_loss;test_accuracy;timestamp\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "Test set: Average loss: 2.3075, Accuracy: 396/3603 (11%)\n",
      "\n",
      "tensor([[  0.,   0.,   0.,  65.,   0., 120.,   1.,   0.,  11.,   0.],\n",
      "        [  0.,   0.,   0.,  97.,   0.,  99.,   0.,   0.,   4.,   0.],\n",
      "        [  0.,   0.,   0., 419.,   0., 661.,   0.,   0.,   2.,   0.],\n",
      "        [  0.,   0.,   0., 321.,   0., 200.,   0.,   0.,   9.,   0.],\n",
      "        [  0.,   0.,   0., 134.,   0., 281.,   0.,   0.,  29.,   0.],\n",
      "        [  0.,   0.,   0.,   7.,   0.,  75.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  19.,   0., 101.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 246.,   0.,  18.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   4.,   0.,  31.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 294.,   0., 323.,   0.,   0.,  32.,   0.]])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.6057, 0.0000, 0.9146, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "Epoch 2\n",
      "\n",
      "Test set: Average loss: 2.3075, Accuracy: 396/3603 (11%)\n",
      "\n",
      "tensor([[  0.,   0.,   0.,  65.,   0., 120.,   1.,   0.,  11.,   0.],\n",
      "        [  0.,   0.,   0.,  97.,   0.,  99.,   0.,   0.,   4.,   0.],\n",
      "        [  0.,   0.,   0., 419.,   0., 661.,   0.,   0.,   2.,   0.],\n",
      "        [  0.,   0.,   0., 321.,   0., 200.,   0.,   0.,   9.,   0.],\n",
      "        [  0.,   0.,   0., 134.,   0., 281.,   0.,   0.,  29.,   0.],\n",
      "        [  0.,   0.,   0.,   7.,   0.,  75.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  19.,   0., 101.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 246.,   0.,  18.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   4.,   0.,  31.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 294.,   0., 323.,   0.,   0.,  32.,   0.]])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.6057, 0.0000, 0.9146, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "Epoch 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1636dac6fb7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m#test normal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mcsv_normal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\";\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\";\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-585be18bdbb5>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(device, test_loader, length_of_dataset)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m    137\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2637\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2638\u001b[1;33m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2639\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2640\u001b[0m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Write to file:\n",
    "dateString = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "f= open((\"exp_traffic_\"+dateString+\".txt\"),\"w+\")\n",
    "\n",
    "#EXP-setup\n",
    "csv_header = \"#merge strategy: \" + \"smpc\" + \"\\n\"\n",
    "csv_header += \"#number of distributed sources: \" + str(len(compute_nodes) + len(frauds)) + \"\\n\"\n",
    "csv_header += \"#batch size: \" + str(batch_size) + \"\\n\"\n",
    "csv_header += \"#distribution of data: \" + \"equally distributed subset\" + \"\\n\"\n",
    "csv_header += \"#percentage of backdoored nodes: \" + str(len(frauds)/(len(compute_nodes) + len(frauds))) + \"\\n\"\n",
    "csv_header += \"#percentage of poisoned data in backdoored nodes: \" + str(percentagePoisonedData*100) + \"\\n\"\n",
    "csv_header += \"#way backdoor looks like: \" + \"green_squares\" + \"\\n\"\n",
    "csv_header += \"#order of time: \" + \"together\" + \"\\n\"\n",
    "csv_header += \"#attack model: \" + (\"model replacement\" if modelReplacement else \"basic\") + \"\\n\"\n",
    "csv_header += \"#starttime: \" + datetime.datetime.now().strftime(\"%H%M%S\") + \"\\n\"\n",
    "csv_header += \"training_type;epoch_number;learn_rate;avg_training_loss;avg_test_loss;test_accuracy;timestamp\" + \"\\n\"\n",
    "print(csv_header)\n",
    "f.write(csv_header)\n",
    "f.close()\n",
    "\n",
    "\n",
    "#RUN training\n",
    "for epoch in range(1, 201):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    \n",
    "    csv_normal = \"normal;\" + str(epoch) + \";\" + str(0.01) + \";\"\n",
    "    csv_backdoor = \"backdoor;\" + str(epoch) + \";\" + str(0.01) + \";\"\n",
    "\n",
    "    \n",
    "    #train both - set order in method\n",
    "    avg_training_backdoor_loss = 0 #TODO\n",
    "    trainSMPC(epoch, modelReplacement)\n",
    "    csv_normal += str(avg_training_backdoor_loss) + \";\"\n",
    "    csv_backdoor += str(avg_training_backdoor_loss) + \";\"\n",
    "    timestamp_combined = datetime.datetime.now().strftime(\"%H%M%S\")\n",
    "    \n",
    "    #save after each 10 iterations\n",
    "    if epoch % 2 == 0:\n",
    "        torch.save(models[0].state_dict(), (\"exp_traffic_\"+dateString +\"_epoch_\" + str(epoch) + \".pt\"))\n",
    "    \n",
    "    #test backdoor\n",
    "    #test_loss, acc = test(device, dataset_loader_backdoored_test, len(backdoored_test))\n",
    "    #csv_backdoor += str(test_loss) + \";\" + acc + \";\"\n",
    "    #\n",
    "    #test normal\n",
    "    test_loss, acc = test(device, test_loader, len(testdata))\n",
    "    csv_normal += str(test_loss) + \";\" + acc + \";\"\n",
    "    #\n",
    "    ##timestamps\n",
    "    csv_normal += timestamp_combined + \"\\n\"\n",
    "    csv_backdoor += timestamp_combined + \"\\n\"\n",
    "    \n",
    "    #Write to file\n",
    "    f= open((\"exp_traffic_\"+dateString+\".txt\"),\"a+\")\n",
    "    f.write(csv_backdoor)\n",
    "    f.write(csv_normal)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
