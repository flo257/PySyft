{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import syft as sy\n",
    "import sys\n",
    "import pdb \n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import datetime\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20e8780e630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "device = torch.device(\"cuda\")\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "image_size = (32,32)\n",
    "seed = 10\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice\n",
    "compute_nodes = [alice, bob]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Train CIFAR 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Test CIFAR 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send dataset to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor>[PointerTensor | me:6823191096 -> alice:18083011846],\n",
       " Tensor>[PointerTensor | me:67442683695 -> alice:63166093778])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_distributed_dataset = []\n",
    "#normal clients\n",
    "for batch_idx, (data,target) in enumerate(trainloader):\n",
    "            data_append = data.send(compute_nodes[batch_idx % len(compute_nodes)], inplace = True)\n",
    "            target_append = target.send(compute_nodes[batch_idx % len(compute_nodes)], inplace = True)\n",
    "            train_distributed_dataset.append((data_append, target_append))\n",
    "\n",
    "#shuffle list\n",
    "shuffle(train_distributed_dataset)\n",
    "\n",
    "#train_distributed_dataset[1] - to check that it's shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, device, trainloader):\n",
    "    for batch_idx, (data,target) in enumerate(trainloader):        \n",
    "        model.send(data.location) # 0) send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # 1) erase previous gradients (if they exist)\n",
    "\n",
    "        output = model(data)  # 2) make a prediction\n",
    "        loss = F.nll_loss(output, target) # 3) calculate how much we missed\n",
    "        loss.backward() # 4) figure out which weights caused us to miss\n",
    "        optimizer.step() # 5) change those weights\n",
    "        model.get() # 6) get model (with gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everyting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 17:18:56.899167\n",
      "epoch: 1\n",
      "Test set: Average loss: 1.9996, Accuracy: 2829/10000 (28%)\n",
      "\n",
      "2019-06-18 17:19:46.818082\n",
      "epoch: 2\n",
      "Test set: Average loss: 1.7568, Accuracy: 3690/10000 (37%)\n",
      "\n",
      "2019-06-18 17:20:36.223179\n",
      "epoch: 3\n",
      "Test set: Average loss: 1.5920, Accuracy: 4182/10000 (42%)\n",
      "\n",
      "2019-06-18 17:21:25.774209\n",
      "epoch: 4\n",
      "Test set: Average loss: 1.4877, Accuracy: 4619/10000 (46%)\n",
      "\n",
      "2019-06-18 17:22:15.320244\n",
      "epoch: 5\n",
      "Test set: Average loss: 1.4223, Accuracy: 4863/10000 (49%)\n",
      "\n",
      "2019-06-18 17:23:04.653369\n",
      "epoch: 6\n",
      "Test set: Average loss: 1.3801, Accuracy: 5042/10000 (50%)\n",
      "\n",
      "2019-06-18 17:23:53.974455\n",
      "epoch: 7\n",
      "Test set: Average loss: 1.3476, Accuracy: 5129/10000 (51%)\n",
      "\n",
      "2019-06-18 17:24:43.710429\n",
      "epoch: 8\n",
      "Test set: Average loss: 1.3160, Accuracy: 5269/10000 (53%)\n",
      "\n",
      "2019-06-18 17:25:32.780617\n",
      "epoch: 9\n",
      "Test set: Average loss: 1.2881, Accuracy: 5383/10000 (54%)\n",
      "\n",
      "2019-06-18 17:26:21.937778\n",
      "epoch: 10\n",
      "Test set: Average loss: 1.2720, Accuracy: 5433/10000 (54%)\n",
      "\n",
      "2019-06-18 17:27:11.218915\n",
      "epoch: 11\n",
      "Test set: Average loss: 1.2486, Accuracy: 5508/10000 (55%)\n",
      "\n",
      "2019-06-18 17:28:00.164127\n",
      "epoch: 12\n",
      "Test set: Average loss: 1.2290, Accuracy: 5576/10000 (56%)\n",
      "\n",
      "2019-06-18 17:28:49.248321\n",
      "epoch: 13\n",
      "Test set: Average loss: 1.2150, Accuracy: 5627/10000 (56%)\n",
      "\n",
      "2019-06-18 17:29:38.504440\n",
      "epoch: 14\n",
      "Test set: Average loss: 1.2063, Accuracy: 5680/10000 (57%)\n",
      "\n",
      "2019-06-18 17:30:27.936927\n",
      "epoch: 15\n",
      "Test set: Average loss: 1.1903, Accuracy: 5723/10000 (57%)\n",
      "\n",
      "2019-06-18 17:31:17.593926\n",
      "epoch: 16\n",
      "Test set: Average loss: 1.1768, Accuracy: 5766/10000 (58%)\n",
      "\n",
      "2019-06-18 17:32:07.807747\n",
      "epoch: 17\n",
      "Test set: Average loss: 1.1653, Accuracy: 5807/10000 (58%)\n",
      "\n",
      "2019-06-18 17:32:56.876935\n",
      "epoch: 18\n",
      "Test set: Average loss: 1.1555, Accuracy: 5880/10000 (59%)\n",
      "\n",
      "2019-06-18 17:33:48.772213\n",
      "epoch: 19\n",
      "Test set: Average loss: 1.1410, Accuracy: 5941/10000 (59%)\n",
      "\n",
      "2019-06-18 17:34:38.373230\n",
      "epoch: 20\n",
      "Test set: Average loss: 1.1386, Accuracy: 5958/10000 (60%)\n",
      "\n",
      "2019-06-18 17:35:27.485405\n",
      "epoch: 21\n",
      "Test set: Average loss: 1.1353, Accuracy: 5972/10000 (60%)\n",
      "\n",
      "2019-06-18 17:36:16.770525\n",
      "epoch: 22\n",
      "Test set: Average loss: 1.1313, Accuracy: 5990/10000 (60%)\n",
      "\n",
      "2019-06-18 17:37:06.160632\n",
      "epoch: 23\n",
      "Test set: Average loss: 1.1265, Accuracy: 6008/10000 (60%)\n",
      "\n",
      "2019-06-18 17:37:55.459742\n",
      "epoch: 24\n",
      "Test set: Average loss: 1.1264, Accuracy: 6028/10000 (60%)\n",
      "\n",
      "2019-06-18 17:38:45.150713\n",
      "epoch: 25\n",
      "Test set: Average loss: 1.1222, Accuracy: 6046/10000 (60%)\n",
      "\n",
      "2019-06-18 17:39:34.598780\n",
      "epoch: 26\n",
      "Test set: Average loss: 1.1224, Accuracy: 6066/10000 (61%)\n",
      "\n",
      "2019-06-18 17:40:23.882899\n",
      "epoch: 27\n",
      "Test set: Average loss: 1.1195, Accuracy: 6080/10000 (61%)\n",
      "\n",
      "2019-06-18 17:41:13.774002\n",
      "epoch: 28\n",
      "Test set: Average loss: 1.1244, Accuracy: 6094/10000 (61%)\n",
      "\n",
      "2019-06-18 17:42:03.567958\n",
      "epoch: 29\n",
      "Test set: Average loss: 1.1265, Accuracy: 6092/10000 (61%)\n",
      "\n",
      "2019-06-18 17:42:52.987034\n",
      "epoch: 30\n",
      "Test set: Average loss: 1.1309, Accuracy: 6086/10000 (61%)\n",
      "\n",
      "2019-06-18 17:43:42.520091\n",
      "epoch: 31\n",
      "Test set: Average loss: 1.1379, Accuracy: 6107/10000 (61%)\n",
      "\n",
      "2019-06-18 17:44:31.736214\n",
      "epoch: 32\n",
      "Test set: Average loss: 1.1440, Accuracy: 6100/10000 (61%)\n",
      "\n",
      "2019-06-18 17:45:21.539167\n",
      "epoch: 33\n",
      "Test set: Average loss: 1.1559, Accuracy: 6081/10000 (61%)\n",
      "\n",
      "2019-06-18 17:46:10.819287\n",
      "epoch: 34\n",
      "Test set: Average loss: 1.1577, Accuracy: 6090/10000 (61%)\n",
      "\n",
      "2019-06-18 17:47:00.122401\n",
      "epoch: 35\n",
      "Test set: Average loss: 1.1668, Accuracy: 6081/10000 (61%)\n",
      "\n",
      "2019-06-18 17:47:49.769403\n",
      "epoch: 36\n",
      "Test set: Average loss: 1.1684, Accuracy: 6111/10000 (61%)\n",
      "\n",
      "2019-06-18 17:48:38.908587\n",
      "epoch: 37\n",
      "Test set: Average loss: 1.1727, Accuracy: 6120/10000 (61%)\n",
      "\n",
      "2019-06-18 17:49:28.134708\n",
      "epoch: 38\n",
      "Test set: Average loss: 1.1785, Accuracy: 6105/10000 (61%)\n",
      "\n",
      "2019-06-18 17:50:17.506816\n",
      "epoch: 39\n",
      "Test set: Average loss: 1.1930, Accuracy: 6097/10000 (61%)\n",
      "\n",
      "2019-06-18 17:51:06.400050\n",
      "epoch: 40\n",
      "Test set: Average loss: 1.2031, Accuracy: 6098/10000 (61%)\n",
      "\n",
      "2019-06-18 17:51:55.417249\n",
      "epoch: 41\n",
      "Test set: Average loss: 1.2128, Accuracy: 6104/10000 (61%)\n",
      "\n",
      "2019-06-18 17:52:44.571429\n",
      "epoch: 42\n",
      "Test set: Average loss: 1.2248, Accuracy: 6092/10000 (61%)\n",
      "\n",
      "2019-06-18 17:53:33.474687\n",
      "epoch: 43\n",
      "Test set: Average loss: 1.2373, Accuracy: 6071/10000 (61%)\n",
      "\n",
      "2019-06-18 17:54:21.883088\n",
      "epoch: 44\n",
      "Test set: Average loss: 1.2467, Accuracy: 6065/10000 (61%)\n",
      "\n",
      "2019-06-18 17:55:10.574399\n",
      "epoch: 45\n",
      "Test set: Average loss: 1.2604, Accuracy: 6043/10000 (60%)\n",
      "\n",
      "2019-06-18 17:55:59.566716\n",
      "epoch: 46\n",
      "Test set: Average loss: 1.2844, Accuracy: 5982/10000 (60%)\n",
      "\n",
      "2019-06-18 17:56:48.407977\n",
      "epoch: 47\n",
      "Test set: Average loss: 1.3059, Accuracy: 5980/10000 (60%)\n",
      "\n",
      "2019-06-18 17:57:37.088292\n",
      "epoch: 48\n",
      "Test set: Average loss: 1.3284, Accuracy: 5945/10000 (59%)\n",
      "\n",
      "2019-06-18 17:58:25.815590\n",
      "epoch: 49\n",
      "Test set: Average loss: 1.3264, Accuracy: 5970/10000 (60%)\n",
      "\n",
      "2019-06-18 17:59:14.398935\n",
      "epoch: 50\n",
      "Test set: Average loss: 1.3445, Accuracy: 5963/10000 (60%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"epoch: \" + str(epoch))\n",
    "    train(epoch, device, train_distributed_dataset)\n",
    "    test(model, device, testloader)\n",
    "#save model\n",
    "torch.save(model.state_dict(), (\"cifar_model.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
