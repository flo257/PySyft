{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import syft as sy\n",
    "import sys\n",
    "import pdb \n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuda = False\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 100\n",
    "image_size = (32,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "#chalie = sy.VirtualWorker(hook, id=\"chalie\")  # <-- NEW: define remote worker bob\n",
    "#dave = sy.VirtualWorker(hook, id=\"dave\")  # <-- NEW: define remote worker bob\n",
    "#evelyn = sy.VirtualWorker(hook, id=\"evelyn\")  # <-- NEW: define remote worker bob\n",
    "#frank = sy.VirtualWorker(hook, id=\"frank\")  # <-- NEW: define remote worker bob\n",
    "#gustav = sy.VirtualWorker(hook, id=\"gustav\")  # <-- NEW: define remote worker bob\n",
    "#helmut = sy.VirtualWorker(hook, id=\"helmut\")  # <-- NEW: define remote worker bob\n",
    "#isabella = sy.VirtualWorker(hook, id=\"isabella\")  # <-- NEW: define remote worker bob\n",
    "\n",
    "\n",
    "fraudulin = sy.VirtualWorker(hook, id=\"fraudulin\")  # <-- NEW: and alice\n",
    "#fraudrich = sy.VirtualWorker(hook, id=\"fraudrich\")  # <-- NEW: and alice\n",
    "\n",
    "\n",
    "compute_nodes = [alice, bob]\n",
    "frauds = [fraudulin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "gtsrd = datasets.ImageFolder(root = \n",
    "                             'C:\\\\Users\\Florian\\\\Desktop\\\\testing_backdoor\\\\real',\n",
    "                             transform=data_transform)\n",
    "dataset_loader = torch.utils.data.DataLoader(gtsrd,\n",
    "                                             batch_size=batch_size, shuffle=True,\n",
    "                                              **kwargs)\n",
    "\n",
    "test = datasets.ImageFolder(root = \n",
    "                             'C:\\\\Users\\Florian\\\\Desktop\\\\testing_backdoor\\\\test',\n",
    "                             transform=data_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test,\n",
    "                                             batch_size=batch_size, shuffle=True,\n",
    "                                              **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsrd_backdoored = datasets.ImageFolder(root = \n",
    "                             'C:\\\\Users\\Florian\\\\Desktop\\\\testing_backdoor\\\\fake',\n",
    "                             transform=data_transform)\n",
    "dataset_loader_backdoored = torch.utils.data.DataLoader(gtsrd_backdoored,\n",
    "                                             batch_size=batch_size, shuffle=True,\n",
    "                                              **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6c3c8cdb58b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgtsrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Get a batch of training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Make a grid from batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    582\u001b[0m             \u001b[1;31m# prime the prefetch loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_put_indices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\florian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Letâ€™s visualize a few training images so as to understand the data augmentations.\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "class_names = gtsrd.classes\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataset_loader))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5) #kernel size = filter size\n",
    "        self.conv1 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2,stride=2)           #First Max-Pooling Layer\n",
    "        self.conv2 = nn.Conv2d(32, 96, 3)\n",
    "        self.conv3 = nn.Conv2d(96, 256, 3)\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.37)\n",
    "        self.fc0 = nn.Linear(4096,2048)            #First Fully-Connected Layer\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 43)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        x = F.relu(self.conv0(x))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 4096)\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "#http://publications.lib.chalmers.se/records/fulltext/255863/255863.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send dataset to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset = []\n",
    "#normal clients\n",
    "for batch_idx, (data,target) in enumerate(dataset_loader):\n",
    "            data_append = data.send(compute_nodes[batch_idx % len(compute_nodes)], inplace = True)\n",
    "            target_append = target.send(compute_nodes[batch_idx % len(compute_nodes)], inplace = True)\n",
    "            train_distributed_dataset.append((data_append, target_append))\n",
    "      \n",
    "#backdoored clients   \n",
    "for batch_idx, (data,target) in enumerate(dataset_loader_backdoored):\n",
    "            data_append = data.send(frauds[batch_idx % len(frauds)], inplace = True)\n",
    "            target_append = target.send(frauds[batch_idx % len(frauds)], inplace = True)\n",
    "            train_distributed_dataset.append((data_append, target_append))\n",
    "            \n",
    "#shuffle list\n",
    "shuffle(train_distributed_dataset)\n",
    "\n",
    "train_distributed_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline attack\n",
    "def train(epoch):\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):        \n",
    "        model.send(data.location) # 0) send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() # 1) erase previous gradients (if they exist)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        output = model(data)  # 2) make a prediction\n",
    "        loss = F.nll_loss(output, target) # 3) calculate how much we missed\n",
    "        loss.backward() # 4) figure out which weights caused us to miss\n",
    "        optimizer.step() # 5) change those weights\n",
    "        model.get() # 6) get model (with gradients)\n",
    "            \n",
    "        #if batch_idx % 100 == 0:\n",
    "        loss = loss.get() # <-- NEW: get the loss back\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_distributed_dataset) * batch_size,\n",
    "                100. * batch_idx / len(train_distributed_dataset), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everyting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "# stopped at:\n",
    "#model.load_state_dict(torch.load(\"gtsdb_paper_2.pt\"))\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "    # Assuming optimizer uses lr = 0.05 for all groups\n",
    "    # lr = 0.05     if epoch < 30\n",
    "    # lr = 0.005    if 30 <= epoch < 80\n",
    "    # lr = 0.0005   if epoch >= 80\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "#RUN training\n",
    "for epoch in range(1, 50):\n",
    "    #scheduler.step()\n",
    "    train(epoch)\n",
    "    #torch.save(model.state_dict(), (\"gtsdb_paper_\" + str(epoch) + \".pt\"))\n",
    "    test(model, device, test_loader)\n",
    "    \n",
    "\n",
    "#save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtsdb_model = Net()\n",
    "gtsdb_model.load_state_dict(torch.load(\"gtsdb_paper_2.pt\"))\n",
    "test(gtsdb_model, device, dataset_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
