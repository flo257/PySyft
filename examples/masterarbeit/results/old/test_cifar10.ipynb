{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import syft as sy\n",
    "import sys\n",
    "import pdb \n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import datetime\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e1eb5ac630>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "device = torch.device(\"cuda\")\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "image_size = (32,32)\n",
    "seed = 10\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Train CIFAR 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Test CIFAR 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, device, trainloader):\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = F.nll_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everyting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 16:51:20.945481\n",
      "epoch: 1\n",
      "Test set: Average loss: 1.9809, Accuracy: 2837/10000 (28%)\n",
      "\n",
      "2019-06-18 16:51:34.390148\n",
      "epoch: 2\n",
      "Test set: Average loss: 1.7325, Accuracy: 3665/10000 (37%)\n",
      "\n",
      "2019-06-18 16:51:46.978085\n",
      "epoch: 3\n",
      "Test set: Average loss: 1.6315, Accuracy: 3970/10000 (40%)\n",
      "\n",
      "2019-06-18 16:51:59.787958\n",
      "epoch: 4\n",
      "Test set: Average loss: 1.8664, Accuracy: 2966/10000 (30%)\n",
      "\n",
      "2019-06-18 16:52:12.272298\n",
      "epoch: 5\n",
      "Test set: Average loss: 1.4561, Accuracy: 4684/10000 (47%)\n",
      "\n",
      "2019-06-18 16:52:24.469368\n",
      "epoch: 6\n",
      "Test set: Average loss: 1.5562, Accuracy: 4454/10000 (45%)\n",
      "\n",
      "2019-06-18 16:52:36.694437\n",
      "epoch: 7\n",
      "Test set: Average loss: 1.4192, Accuracy: 4864/10000 (49%)\n",
      "\n",
      "2019-06-18 16:52:48.854515\n",
      "epoch: 8\n",
      "Test set: Average loss: 1.4128, Accuracy: 4944/10000 (49%)\n",
      "\n",
      "2019-06-18 16:53:00.989606\n",
      "epoch: 9\n",
      "Test set: Average loss: 1.4913, Accuracy: 4754/10000 (48%)\n",
      "\n",
      "2019-06-18 16:53:13.165682\n",
      "epoch: 10\n",
      "Test set: Average loss: 1.4189, Accuracy: 5014/10000 (50%)\n",
      "\n",
      "2019-06-18 16:53:25.394741\n",
      "epoch: 11\n",
      "Test set: Average loss: 1.3440, Accuracy: 5185/10000 (52%)\n",
      "\n",
      "2019-06-18 16:53:37.767755\n",
      "epoch: 12\n",
      "Test set: Average loss: 1.3332, Accuracy: 5185/10000 (52%)\n",
      "\n",
      "2019-06-18 16:53:49.993815\n",
      "epoch: 13\n",
      "Test set: Average loss: 1.2869, Accuracy: 5304/10000 (53%)\n",
      "\n",
      "2019-06-18 16:54:02.162894\n",
      "epoch: 14\n",
      "Test set: Average loss: 1.2631, Accuracy: 5439/10000 (54%)\n",
      "\n",
      "2019-06-18 16:54:14.395951\n",
      "epoch: 15\n",
      "Test set: Average loss: 1.2594, Accuracy: 5452/10000 (55%)\n",
      "\n",
      "2019-06-18 16:54:26.601036\n",
      "epoch: 16\n",
      "Test set: Average loss: 1.1729, Accuracy: 5820/10000 (58%)\n",
      "\n",
      "2019-06-18 16:54:38.780112\n",
      "epoch: 17\n",
      "Test set: Average loss: 1.2490, Accuracy: 5531/10000 (55%)\n",
      "\n",
      "2019-06-18 16:54:50.869217\n",
      "epoch: 18\n",
      "Test set: Average loss: 1.2204, Accuracy: 5779/10000 (58%)\n",
      "\n",
      "2019-06-18 16:55:02.963320\n",
      "epoch: 19\n",
      "Test set: Average loss: 1.1993, Accuracy: 5715/10000 (57%)\n",
      "\n",
      "2019-06-18 16:55:15.196396\n",
      "epoch: 20\n",
      "Test set: Average loss: 1.2429, Accuracy: 5620/10000 (56%)\n",
      "\n",
      "2019-06-18 16:55:27.386458\n",
      "epoch: 21\n",
      "Test set: Average loss: 1.1826, Accuracy: 5831/10000 (58%)\n",
      "\n",
      "2019-06-18 16:55:39.608512\n",
      "epoch: 22\n",
      "Test set: Average loss: 1.0865, Accuracy: 6154/10000 (62%)\n",
      "\n",
      "2019-06-18 16:55:51.793586\n",
      "epoch: 23\n",
      "Test set: Average loss: 1.1577, Accuracy: 5900/10000 (59%)\n",
      "\n",
      "2019-06-18 16:56:03.944670\n",
      "epoch: 24\n",
      "Test set: Average loss: 1.1543, Accuracy: 5944/10000 (59%)\n",
      "\n",
      "2019-06-18 16:56:16.177728\n",
      "epoch: 25\n",
      "Test set: Average loss: 1.0951, Accuracy: 6162/10000 (62%)\n",
      "\n",
      "2019-06-18 16:56:28.346807\n",
      "epoch: 26\n",
      "Test set: Average loss: 1.1074, Accuracy: 6100/10000 (61%)\n",
      "\n",
      "2019-06-18 16:56:40.560871\n",
      "epoch: 27\n",
      "Test set: Average loss: 1.0749, Accuracy: 6202/10000 (62%)\n",
      "\n",
      "2019-06-18 16:56:52.776945\n",
      "epoch: 28\n",
      "Test set: Average loss: 1.0862, Accuracy: 6182/10000 (62%)\n",
      "\n",
      "2019-06-18 16:57:04.915029\n",
      "epoch: 29\n",
      "Test set: Average loss: 1.0546, Accuracy: 6248/10000 (62%)\n",
      "\n",
      "2019-06-18 16:57:17.065109\n",
      "epoch: 30\n",
      "Test set: Average loss: 1.0427, Accuracy: 6300/10000 (63%)\n",
      "\n",
      "2019-06-18 16:57:29.343153\n",
      "epoch: 31\n",
      "Test set: Average loss: 1.0732, Accuracy: 6239/10000 (62%)\n",
      "\n",
      "2019-06-18 16:57:41.821390\n",
      "epoch: 32\n",
      "Test set: Average loss: 1.0427, Accuracy: 6333/10000 (63%)\n",
      "\n",
      "2019-06-18 16:57:53.989469\n",
      "epoch: 33\n",
      "Test set: Average loss: 1.0831, Accuracy: 6186/10000 (62%)\n",
      "\n",
      "2019-06-18 16:58:06.201534\n",
      "epoch: 34\n",
      "Test set: Average loss: 1.0955, Accuracy: 6131/10000 (61%)\n",
      "\n",
      "2019-06-18 16:58:18.445589\n",
      "epoch: 35\n",
      "Test set: Average loss: 1.0369, Accuracy: 6405/10000 (64%)\n",
      "\n",
      "2019-06-18 16:58:30.787612\n",
      "epoch: 36\n",
      "Test set: Average loss: 1.0828, Accuracy: 6164/10000 (62%)\n",
      "\n",
      "2019-06-18 16:58:43.001677\n",
      "epoch: 37\n",
      "Test set: Average loss: 1.1667, Accuracy: 5900/10000 (59%)\n",
      "\n",
      "2019-06-18 16:58:55.218740\n",
      "epoch: 38\n",
      "Test set: Average loss: 1.0248, Accuracy: 6325/10000 (63%)\n",
      "\n",
      "2019-06-18 16:59:07.311843\n",
      "epoch: 39\n",
      "Test set: Average loss: 1.0135, Accuracy: 6389/10000 (64%)\n",
      "\n",
      "2019-06-18 16:59:19.499916\n",
      "epoch: 40\n",
      "Test set: Average loss: 0.9829, Accuracy: 6538/10000 (65%)\n",
      "\n",
      "2019-06-18 16:59:31.785957\n",
      "epoch: 41\n",
      "Test set: Average loss: 1.0642, Accuracy: 6239/10000 (62%)\n",
      "\n",
      "2019-06-18 16:59:43.995023\n",
      "epoch: 42\n",
      "Test set: Average loss: 1.0567, Accuracy: 6251/10000 (63%)\n",
      "\n",
      "2019-06-18 16:59:56.198090\n",
      "epoch: 43\n",
      "Test set: Average loss: 1.0198, Accuracy: 6409/10000 (64%)\n",
      "\n",
      "2019-06-18 17:00:08.482132\n",
      "epoch: 44\n",
      "Test set: Average loss: 1.0506, Accuracy: 6269/10000 (63%)\n",
      "\n",
      "2019-06-18 17:00:21.719866\n",
      "epoch: 45\n",
      "Test set: Average loss: 1.0237, Accuracy: 6409/10000 (64%)\n",
      "\n",
      "2019-06-18 17:00:34.405930\n",
      "epoch: 46\n",
      "Test set: Average loss: 0.9739, Accuracy: 6532/10000 (65%)\n",
      "\n",
      "2019-06-18 17:00:46.957307\n",
      "epoch: 47\n",
      "Test set: Average loss: 1.1220, Accuracy: 6052/10000 (61%)\n",
      "\n",
      "2019-06-18 17:00:59.244688\n",
      "epoch: 48\n",
      "Test set: Average loss: 0.9745, Accuracy: 6564/10000 (66%)\n",
      "\n",
      "2019-06-18 17:01:11.446757\n",
      "epoch: 49\n",
      "Test set: Average loss: 0.9570, Accuracy: 6691/10000 (67%)\n",
      "\n",
      "2019-06-18 17:01:23.658825\n",
      "epoch: 50\n",
      "Test set: Average loss: 1.0309, Accuracy: 6386/10000 (64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"epoch: \" + str(epoch))\n",
    "    train(epoch, device, trainloader)\n",
    "    test(model, device, testloader)\n",
    "#save model\n",
    "torch.save(model.state_dict(), (\"cifar_model.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
